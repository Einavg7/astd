# Analysis of spatio-temporal data, WS 2019/20, Oct 21

```{r echo=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
set.seed(1331) # make reproduction static
```


## Reminder

1. What is correlation? $$r(x,y) = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2 \sum_{i=1}^n(y_i-\bar{y})^2}}$$
    * symmetric
    * measuring linear relationship
    * bound by [-1,1]
1. What is linear regression? $$y = b_0 + b_1 x + e$$
    * asymmetric
    * $y$ is dependent (random), $x$ is independent (non-random)
    * $b_0$ and $b_1$ are not bound, but have measurement units of $y$ ($b_0$) or relating $x$ to $y$ ($b_1$)




## Literature

The first two articles in [R News
2/2](https://www.r-project.org/doc/Rnews/Rnews_2002-2.pdf), "Time
Series in R 1.5.0" and "Naive Time Series Forecasting Methods"
provide some introduction to the topics discussed below. A more
extensive text, focused on forecasting, is Hyndman and 
Athanasopoulos' [Forecasting: Principles and Practice](https://otexts.org/fpp2/)

## Time series: time domain, autocorrelation, AR, MA

We disinguish continuous time processes 

$$y(t), t \in [T_1,T_2]$$ 

from time events 

$$\\{t_1,t_2,...,t_n\\} t \in [T_1,T_2].$$ 

Regular time series data are samples (or aggregations over a time interval) $y(t_i)$ taken from the process, with $t_{i+1}-t_i$  constant: the time interval. If this is the case, we typically write $y(t_i)$ as $y_i$.

We will now give examples using datasets available in R.

An example of the first type is

```{r fig.width=9}
print(co2,digits=3)
plot(co2)
```

It concerns monthly values of CO$_2$ in the air, measured at the Mauna Loa volcano, starting in 1959. See here [here](ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt) for more recent data.

Another data set gives times of events of the Old Faithful geyser in Yellowstone National Park (USA); the first six records are
```{r}
head(faithful)
```
the first variable is the eruption duration (mins), the second the time to the next eruption. This means that we get actual times $t_i$ 
```{r}
time.mins <- cumsum(apply(faithful, 1, sum))
time.mins[1:10]
```
where time `0` refers to the start of the observations. This dataset is a classic example of a (bivariate) bimodal distribution:
```{r}
hist(faithful[,2])
plot(faithful)
```

and even a bivariate bimodal distribution:
```{r}
plot(faithful[, -3], main = "faithful data: Eruptions of Old Faithful",
  xlab = "Eruption time (min)",
  ylab = "Waiting time to next eruption (min)")
```

We will concentrate here, on time series of the first kind.

## Ozone

Ozone data from the `airquality` dataset have missing values:
```{r}
plot(airquality$Ozone, type = 'l')
```

We can remove these by
```{r}
oz = na.omit(airquality$Ozone)
plot(oz, type = 'l')
```

which essentially "glues" the series together, horizontally. This is **not**, in general, a good idea. Try to think of reasons why not.

The series obtained looks rather messy.  A question that arises,
is whether this series can be considered to be purely random,
or whether there is some structure in it.

We can generate similar series that are unstructured by randomly
resampling the same variable; function `sample` does this:

```{r fig.width=12, fig.height=4}
par(mfrow = c(1,3))
plot(sample(oz), type = 'l')
plot(sample(oz), type = 'l')
plot(sample(oz), type = 'l')
par(mfrow = c(1,1))
```

are the resulting patterns similar, or are they more random than the real `Ozone` series?

## Temporal correlation

Temporal correlation investigates whether two observations $y_i$ and $y_{i+1}$ are correlated. We can
compute this correlation, and plot it, by
```{r}
cor(head(oz, -1), tail(oz, -1))
plot(head(oz, -1), tail(oz, -1), asp = 1)
```

The correlation is moderate, but not extremely high. Could it be caused by random chance? We can generate
random correlations _from this dataset_, by generating 1000 correlations of random permutations, and compute
the 2.5 and 97.5 percentiles:
```{r}
corrs = sapply(1:1000, function(x) cor(head(sample(oz), -1), tail(sample(oz), -1)))
quantile(corrs, c(.025, .975))
```

This indicates that a correlation of 0.45 cannot be considered random. The ozone series is temporally correlated.

## Autocorrelation, partial autocorrelation

The above autocorrelation is the lag-1 autocorrelation. We can also compute lag-2, lag-3, ... autocorrelations,
and plot them:
```{r}
acf(oz)
```

where, trivially, the lag-0 autocorrelation is 1. The blue
lines indicate the bounds between which autocorrelation are not
significantly different from zero.

If the lag-1 autocorrelation is high, automatically the lag-2 autocorrelation will be high too. But are $y_i$
and $y_{i+2}$ still correlated if we would correct for the lat-1 autocorrelation? Partial autocorrelation
computes this:

```{r}
pacf(oz)
```

which shows that partial autocorrelation for lag 2 is not significant.

An example where autocorrelation is high, but partial autocorrelation
beyond lag 1 is zero, is that of random walk, where $y_{i+1}=y_i +
e_i$ with $e_i$ an independent random variable:

```{r, fig.width=12, fig.height=4}
par(mfrow = c(1,3))
x = cumsum(rnorm(1000))
plot(x, type = 'l')
acf(x)
pacf(x)
par(mfrow = c(1,1))
```

An example where lag-1 autocorrelation is negative is found in the faithful
data:

```{r}
acf(faithful$waiting) # $
```

In case of a regular periodicity, such as in a sinus function, we see alternating
(periodic) effects in the acf too:

```{r fig.width=10}
par(mfrow = c(1,2))
x = sin(1:300/10)
plot(x, type = 'l')
acf(x, 100)
par(mfrow = c(1,1))
```

Why is the autocorrelation at lag 60 not equal to 1? Try also with a set ranging from 1 to 3000.

## Autoregressive models, moving average models

Autoregressive models relate observations to their previous values:

$$y_i = \sum_{j=1}^p \phi_j y_{i-j} + e_i$$

Moving average models relate observations to a moving average of previous noise values:

$$y_i = \sum_{k=1}^q \theta_k e_{i-k} + e_i$$

## Trend models

Trend effects can be take out by computing differences, recursively. For instance, if the
data contains a mean, $y_t = \mu + e_t$, the first-order differences $y_{t+1}-y_t$ will have
zero mean. If the data contains a linear trend in time, $y_t = \mu + \beta t + e_t$, the
first-order difference will have mean equal to $\beta$, the second order difference,
$(y_{t+2}-y_{t+1})-(y_{t+1}-y_t) = y_{t+2}-2y_{t+1}+y_t$ will have zero mean. We can
define the difference operator, $B^i(y)$ as taking the $i-$th order difference:

$$B^0(y_t) = y_t$$

$$B^1(y_t) = y_{t+1}-y_t$$

$$B^2(y_t) = B^1(B^1(y_t)) = y_{t+2}-2y_{t+1}+y_t$$

Illustration:
```{r}
x = -100:200
y = 50 + 2.0 * x + 0.1 * x^2 + rnorm(301)*50
plot(x, y, type = 'l')
plot(x[-1], diff(y), type = 'l')
plot(x[-(1:2)], diff(diff(y)), type = 'l')
mean(diff(diff(y)))
```
What would you expect as value for the mean of the twice-differenced series?

## arima(p,d,q)

The arima(p,d,q) model is then obtained by fitting an AR(p) model
and an MA(q) model to $d$-th order differences:

$$B^d(y_t) = \sum_{j=1}^p \phi_j y_{i-j} + \sum_{k=1}^q \theta_k e_{i-k} + e_t$$

## Automatic model fitting

A function to autoatically select $p$, $d$ and $q$ and estimate the corresponding
model is found in `forecast`:

```{r}
library(forecast)
auto.arima(oz)
```

## Further reading, next steps

* [Here](https://people.duke.edu/~rnau/411arim.htm) is a more extensive explanation of the arima models, arima modelling and use in prediction/forecasting
* arima models do not include any seasonality
* we can fit known seasonalities using e.g. Holt-Winters models, or stl ("seasonal, trend and irregular components using loess")
* we can identify periodicities by using harmonic analysis


## Exercises

1. generate a random series with white noise added to a second order trend, as was done above; fit an ARIMA model to it using `forecast::auto.arima`, and verify discuss whether the model choses is in accordance to the process you simulated
1. repeat this, but make the second order (parabolic) component so small that it is hardly visible; use again `auto.arima` again and discuss the outcome.
1. use `forecast::auto.arima` to the `co2` time series, and interpret the resulting model and coefficients; do read into the documentation for parameter `seasonal`.
1. repeat this, but switching the fitting of a seasonal component off
1. create an stl decomposition of `co2` with `plot(stl(co2, "periodic"))`, and try to interpret the result
1. use the dataset `air`, obtained by `data(air, package = "spacetime")`; compute the autocorrelation function and partial autocorrelation function for the first air quality station (omitting missing values as was done above)

# Analysis of spatio-temporal data, WS 2018/9, Oct 7

```{r echo=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
set.seed(1331) # make reproduction static
```


## Reminder

1. What is correlation?
1. What is linear regression?

## Literature

The first two articles in [R News
2/2](https://www.r-project.org/doc/Rnews/Rnews_2002-2.pdf), "Time
Series in R 1.5.0" and "Naive Time Series Forecasting Methods"
provide some introduction to the topics discussed below. A more
extensive text, focused on forecasting, is Hyndman and 
Athanasopoulos' [Forecasting: Principles and Prac](https://otexts.org/fpp2/)

## Time series: time domain, autocorrelation, AR, MA

We disinguish continuous time processes 

$$y(t), t \in [T_1,T_2]$$ 

from time events 

$$\\{t_1,t_2,...,t_n\\} t \in [T_1,T_2].$$ 

Regular time series data are samples (or aggregations over a time interval) $y(t_i)$ taken from the process, with $t_{i+1}-t_i$  constant: the time interval. If this is the case, we typically write $y(t_i)$ as $y_i$.

We will now give examples using datasets available in R.

An example of the first type is

```{r fig.width=9}
print(co2,digits=3)
plot(co2)
```

It concerns monthly values of CO$_2$ in the air, measured at the Mauna Loa volcano, starting in 1959. See here [here](ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt) for more recent data.

Another data set gives times of events of the Old Faithful geyser in Yellowstone National Park (USA); the first six records are
```{r}
head(faithful)
```
the first variable is the eruption duration (mins), the second the time to the next eruption. This means that we get actual times $t_i$ 
```{r}
time.mins <- cumsum(apply(faithful, 1, sum))
time.mins[1:10]
```
where time `0` refers to the start of the observations. This dataset is a classic example of a (bivariate) bimodal distribution:
```{r}
hist(faithful[,2])
```

We will concentrate here, on time series of the first kind.

## Ozone

Ozone data from the `airquality` dataset have missing values:
```{r}
plot(airquality$Ozone, type = 'l')
```

We can remove these by
```{r}
oz = na.omit(airquality$Ozone)
plot(oz, type = 'l')
```

which essentially "glues" the series together, horizontally. This is not, in general, a good idea.

The series obtained looks rather messy.  A question that arises,
is whether this series can be considered to be purely random,
or whether there is some structure in it.

We can generate similar series that are unstructured by randomly
resampling the same variable; function `sample` does this:

```{r fig.width=12, fig.height=4}
par(mfrow = c(1,3))
plot(sample(oz), type = 'l')
plot(sample(oz), type = 'l')
plot(sample(oz), type = 'l')
par(mfrow = c(1,1))
```

are the resulting patterns similar, or are they more random than the real `Ozone` series?

## Temporal correlation

Temporal correlation investigates whether two observations $y_i$ and $y_{i+1}$ are correlated. We can
compute this correlation, and plot it, by
```{r}
cor(head(oz, -1), tail(oz, -1))
plot(head(oz, -1), tail(oz, -1))
```

The correlation is moderate, but not extremely high. Could it be caused by random chance? We can generate
random correlations _from this dataset_, by generating 1000 correlations of random permutations, and compute
the 2.5 and 97.5 percentiles:
```{r}
corrs = sapply(1:1000, function(x) cor(head(sample(oz), -1), tail(sample(oz), -1)))
quantile(corrs, c(.025, .975))
```

This indicates that a correlation of 0.45 cannot be considered random. The ozone series is temporally correlated.

## Autocorrelation, partial autocorrelation

The above autocorrelation is the lag-1 autocorrelation. We can also compute lag-2, lag-3, ... autocorrelations,
and plot them:
```{r}
acf(oz)
```

where, trivially, the lag-0 autocorrelation is 1. The blue
lines indicate the bounds between which autocorrelation are not
significantly different from zero.

If the lag-1 autocorrelation is high, automatically the lag-2 autocorrelation will be high too. But are $y_i$
and $y_{i+2}$ still correlated if we would correct for the lat-1 autocorrelation? Partial autocorrelation
computes this:

```{r}
pacf(oz)
```

which shows that partial autocorrelation for lag 2 is not significant.

An example where autocorrelation is high, but partial autocorrelation
beyond lag 1 is zero, is that of random walk, where $y_{i+1}=y_i +
e_i$ with $e_i$ an independent random variable:

```{r, fig.width=12, fig.height=4}
par(mfrow = c(1,3))
x = cumsum(rnorm(1000))
plot(x, type = 'l')
acf(x)
pacf(x)
par(mfrow = c(1,1))
```

An example where lag-1 autocorrelation is negative is found in the faithful
data:

```{r}
acf(faithful$waiting) # $
```

In case of a regular periodicity, such as in a sinus function, we see alternating
(periodic) effects in the acf too:

```{r fig.width=10}
par(mfrow = c(1,2))
x = sin(1:300/10)
plot(x, type = 'l')
acf(x, 100)
par(mfrow = c(1,1))
```

## Autoregressive models, moving average models

Autoregressive models relate observations to their previous values:

$$y_i = \sum_{j=1}^p \phi_j y_{i-j} + e_i$$

Moving average models relate observations to a moving average of previous noise values:

$$y_i = \sum_{k=1}^q \theta_k e_{i-k} + e_i$$

## Trend models

Trend effects can be take out by computing differences, recursively. For instance, if the
data contains a mean, $y_t = \mu + e_t$, the first-order differences $y_{t+1}-y_t$ will have
zero mean. If the data contains a linear trend in time, $y_t = \mu + \beta t + e_t$, the
first-order difference will have mean equal to $\beta$, the second order difference,
$(y_{t+2}-y_{t+1})-(y_{t+1}-y_t) = y_{t+2}-2y_{t+1}+y_t$ will have zero mean. We can
define the difference operator, $B^i(y)$ as taking the $i-$th order difference:

$$B^0(y_t) = y_t$$

$$B^1(y_t) = y_{t+1}-y_t$$

$$B^2(y_t) = B^1(B^1(y_t)) = y_{t+2}-2y_{t+1}+y_t$$

## arima(p,d,q)

The arima(p,d,q) model is then obtained by fitting an AR(p) model
and an MA(q) model to $d$-th order differences:

$$B^d(y_t) = \sum_{j=1}^p \phi_j y_{i-j} + \sum_{k=1}^q \theta_k e_{i-k} + e_t$$

## Automatic model fitting

A function to autoatically select $p$, $d$ and $q$ and estimate the corresponding
model is found in `forecast`:

```{r}
library(forecast)
auto.arima(oz)
```

## Further reading, next steps

* [Here](https://people.duke.edu/~rnau/411arim.htm) is a more extensive explanation of the arima models, arima modelling and use in prediction/forecasting
* arima models do not include any seasonality
* we can fit known seasonalities using e.g. Holt-Winters models, or stl ("seasonal, trend and irregular components using loess")
* we can identify periodicities by using harmonic analysis

